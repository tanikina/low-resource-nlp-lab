{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c90418-634a-4a19-957a-cc4f1a412d32",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tanikina/low-resource-nlp-lab/blob/main/notebooks/Prompting101_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8bcfa7-2c91-47be-9fd4-8a9013b925d5",
   "metadata": {},
   "source": [
    "## 💡 101 Prompting Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3852d-e041-4f2c-8eab-cd5bd7beec39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What are prompts?\n",
    "\n",
    "A prompt is a piece of text inserted in the input examples, so that the original task can be formulated as a language modeling problem.\n",
    "\n",
    "Source: [Language Models are Few-Shot Learners (Brown et al., 2020)](https://openai.com/research/language-models-are-few-shot-learners)\n",
    "\n",
    "<center>\n",
    "<img src=\"images/prompts-vs-fine-tunings.png\" alt=\"Fine-Tuning\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e547ef-a7e5-4188-a3a1-cf65f9d7cca2",
   "metadata": {},
   "source": [
    "### 👑 Good prompt rules\n",
    "\n",
    " - **D**irect & Simple: formulate prompt in a concise and direct way, starting with the simplest and shortest version;\n",
    " \n",
    " - **A**dequate Context: provide some context with examples and make sure that they are sufficient for the task and unambiguous;\n",
    " \n",
    " - **O**ne Task at a Time: do not use the same prompt to solve several tasks simultaneously.\n",
    "\n",
    "Check [the best practices of LLM prompting from HuggingFace](https://huggingface.co/docs/transformers/tasks/prompting#best-practices-of-llm-prompting) for further recommendations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb897db6-59b9-43c9-85c6-c670b1779215",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🤔 What is Chain of Thought Prompting?\n",
    "\n",
    "Chain of Thought (CoT) prompting is a method that \n",
    "\n",
    "Source: [Chain-of-Thought Prompting Elicits Reasoning\n",
    "in Large Language Models (Wei et al., 2022)](https://www.semanticscholar.org/paper/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Wei-Wang/1b6e810ce0afd0dd093f789d2b2742d047e316d5)\n",
    "\n",
    "<center>\n",
    "<img src=\"images/cot-prompting.png\" alt=\"CoT Prompting\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef7fa2-01ed-457c-a00a-a70a450f039f",
   "metadata": {},
   "source": [
    "## Prompting Examples with Flan-T5 🍮 and Mistral <img src=\"images/mistral.png\" width=\"20\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3e2c6-5c90-4ca2-86fc-463c9fe970cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a016a29e-ad61-40e0-9ef6-a1f2dedb8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dessert: chocolate cake\n",
      "the red liquid: tomato soup\n",
      "2nd: pizza\n",
      "the last item: tomato soup\n",
      "chocolate flavour: chocolate cake\n"
     ]
    }
   ],
   "source": [
    "# Flan-T5 with Option Selection\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name_or_path = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "def prompt_with_options(prompt_query, options, max_new_tokens=10):\n",
    "    prompt = f\"\"\"Question: Select the item that corresponds to \"{prompt_query}\" in the following list. Options: * {\" * \".join(options)}\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens = max_new_tokens)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return decoded\n",
    "\n",
    "prompt_queries = [\"the dessert\", \"the red liquid\", \"2nd\", \"the last item\", \"chocolate flavour\"]\n",
    "options = [\"chocolate cake\", \"pizza\", \"tomato soup\"]\n",
    "\n",
    "for prompt_query in prompt_queries:\n",
    "    result = prompt_with_options(prompt_query, options)\n",
    "    print(f\"{prompt_query}: {result}\")\n",
    "    #print(f\"{t:<24} {result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611f1bf0-ab21-4e74-a01c-df32d3b86fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral-GPTQ Example\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3467b5f1-f388-47a3-8c38-d32c0e9cd5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me about low-resource NLP\n",
      "Generated: Tell me about low-resource NLP.\n",
      "\n",
      "Low resource natural language processing (LRNLP) is a subfield of computational linguistics that focuses on developing methods and tools for analyzing languages with limited resources, such as data or annotated corpora. This field aims to make NLP accessible to underrepresented communities by providing them with the necessary tools and techniques to analyze their own languages without relying on external sources. LRNLP has been particularly important in regions where there are many indigenous languages but few resources available for researching these languages. In this article, we will explore some of the challenges faced when working with low-resource\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about low-resource NLP\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.6,\n",
    "    do_sample = True,\n",
    "    top_k=5,\n",
    "    top_p=0.95,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=128,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=2\n",
    ")\n",
    "\n",
    "output = model.generate(**inputs, generation_config=generation_config, pad_token_id=tokenizer.pad_token_id)\n",
    "generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec41291-9af6-4272-abd7-4696f58cc605",
   "metadata": {},
   "source": [
    "## Prompting with LangChain 🦜🔗 + 🤗\n",
    "\n",
    "[https://python.langchain.com/docs/integrations/llms/huggingface_hub](https://python.langchain.com/docs/integrations/llms/huggingface_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3630f330-4ebb-4063-9b30-4b97ae9f4b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e14c54-75c9-4002-955f-f0225f509110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901181b6-1e1d-4f2f-9918-b35d1ca22c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d350b8f8-c96f-4950-b6f3-dbbaf6348b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Which language is spoken in Malta?', 'text': 'scottish'}\n",
      "{'question': 'Welche Sprache spricht man in Österrreich?', 'text': 'German'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "question_de = \"Welche Sprache spricht man in Österrreich?\"\n",
    "question_en = \"Which language is spoken in Malta?\"\n",
    "\n",
    "template_de = \"\"\"Frage: {question}\\nAntwort:\"\"\"\n",
    "template_en = \"\"\"Question: {question}\\nAnswer:\"\"\"\n",
    "\n",
    "prompt_en = PromptTemplate.from_template(template_en)\n",
    "prompt_de = PromptTemplate.from_template(template_de)\n",
    "\n",
    "repo_id = \"google/flan-t5-base\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "model = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt_en, llm=model)\n",
    "\n",
    "print(llm_chain.invoke(question_en))\n",
    "print(llm_chain.invoke(question_de))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4838c4c-2b54-4ed2-9d11-b04871a59a88",
   "metadata": {},
   "source": [
    "## 🎪 Tips & Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedc652-3ca2-428a-b6f0-a5a4578902eb",
   "metadata": {},
   "source": [
    "### Things to keep in mind:\n",
    "- Checking prompt **format of the model**, e.g., whether it requires special tags like [INST]: check the model cards on HF\n",
    "\n",
    "- Checking prompt **format for the task**: see some examples on [https://www.promptingguide.ai/prompts](https://www.promptingguide.ai/prompts)\n",
    "\n",
    "- Dealing with **structured outputs** (e.g., JSON): check out [instructor](https://github.com/jxnl/instructor) library and [LangChain](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "- Filtering out **false starts**: *\"As a language model ...\"*, *\"Being an AI system ...\"*\n",
    "\n",
    "- Removing **verbose text** that does not contribute to the task: re-formulate prompt, use options, provide examples\n",
    "\n",
    "- **Iterative prompting**: always start with the simplest possible version, evaluate on few examples and extend if needed\n",
    "\n",
    "- **Asking back**: ask the same question again if the provided answer does not meet expectations or does not follow the format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
