{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb4479b-0a72-4d1d-bae1-fafbe2ceea71",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tanikina/low-resource-nlp-lab/blob/main/notebooks/OpenPrompt_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7d6a9-0f61-4656-8954-794265bdfd8c",
   "metadata": {},
   "source": [
    "<img src=\"images/open-prompt.png\" alt=\"OpenPrompt\" width=\"200\"/>\n",
    "\n",
    "**OpenPrompt** is a framework for prompt tuning with PLMs. It implements various prompting methods, including templating, verbalizing and optimization strategies and supports many PLMs from HuggingFace.\n",
    "\n",
    "**Documentation:** [https://thunlp.github.io/OpenPrompt/](https://thunlp.github.io/OpenPrompt/)\n",
    "\n",
    "**Examples & Tutorials:** [https://github.com/thunlp/OpenPrompt/tree/main/tutorial](https://github.com/thunlp/OpenPrompt/tree/main/tutorial)\n",
    "\n",
    "**Paper:** [OpenPrompt: An Open-source Framework for Prompt-learning (Ding et al., 2022)](https://aclanthology.org/2022.acl-demo.10.pdf)\n",
    "\n",
    "### OpenPrompt Workflow\n",
    "<img src=\"images/open-prompt-workflow.png\" alt=\"OpenPrompt Workflow\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2097b4-38a3-4c46-bb66-a39c58207bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openprompt\n",
    "!pip install datasets\n",
    "!pip install transformers [torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7fa5aa-4f85-4d0e-87a3-e4365eeb65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptForClassification\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, SoftTemplate\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65336a8e-1f93-4cf1-a32f-8409bbfde5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ceee9d-356b-4236-9e46-c1a2b93dad74",
   "metadata": {},
   "source": [
    "### üóÉÔ∏è Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d990254-a0a2-448f-8876-0b6f65616b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPromptProcessor(DataProcessor):\n",
    "\n",
    "    def __init__(self, label_word2idx: Dict[int, str]):\n",
    "        super().__init__()\n",
    "        self.label_word2idx = label_word2idx\n",
    "        self.idx2label_word = {v: k for k, v in self.label_word2idx.items()}\n",
    "\n",
    "    def get_examples(self, dataset: Dataset) -> List[InputExample]:        \n",
    "        examples = []\n",
    "        full_src_lst = []\n",
    "        full_tgt_lst = []\n",
    "        for i in range(len(dataset)):\n",
    "            full_src_lst.append(dataset[\"speaker\"][i] + \" - \" + dataset[\"text\"][i])\n",
    "            label_idx = dataset[\"label\"][i]\n",
    "            full_tgt_lst.append(self.idx2label_word[label_idx])\n",
    "        for i, (src, tgt, tag) in enumerate(zip(full_src_lst, full_tgt_lst, dataset[\"label\"])):\n",
    "            example = InputExample(guid=str(i), text_a=src, tgt_text=tgt, label=tag)\n",
    "            examples.append(example)\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510b39fe-08ce-4f58-81e7-d9331d06fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": \"24\",\n",
      "  \"label\": 3,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"UGV 1 - Frage: der Einsatzbefehl gilt auch f\\u00fcr UGV 1?\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": \"Nachfragen\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data\n",
    "orig_data = load_dataset(\"DFKI/radr_intents\")\n",
    "train_task_dataset = orig_data[\"train\"] # train_task_dataset = Dataset.from_csv(\"radr_intents/train.csv\")\n",
    "dev_task_dataset = orig_data[\"validation\"] \n",
    "test_task_dataset = orig_data[\"test\"] \n",
    "\n",
    "# Mapping between the labels and their \"verbalization\" (e.g., for label 1 \"Einsatzbefehl\" means \"order\")\n",
    "label_word2idx = {\"Absage\":0, \"Einsatzbefehl\":1, \"Informieren\":2, \"Nachfragen\":3, \"Anruf\":4, \"Antwort\":5, \"Sonstiges\":6, \"Zusage\":7}\n",
    "dp = CustomPromptProcessor(label_word2idx)\n",
    "\n",
    "dataset = dict()\n",
    "dataset[\"train\"] = dp.get_examples(train_task_dataset)\n",
    "dataset[\"validation\"] = dp.get_examples(dev_task_dataset)\n",
    "dataset[\"test\"] = dp.get_examples(test_task_dataset)\n",
    "print(dataset[\"train\"][24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8928429-51e9-4362-a3b8-979b60d3e036",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e021672-344e-4617-af0d-10c0ab1a547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, MT5Config\n",
    "from openprompt import plms\n",
    "from openprompt.plms import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fce546-8eae-4752-a227-bcef8204d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "tokenizing: 2610it [00:00, 2924.09it/s]\n",
      "tokenizing: 310it [00:00, 2845.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setting up the hyperparameters\n",
    "num_epochs = 5\n",
    "pmodel_name = \"tradrz-\"+str(num_epochs)+\"-epochs\"\n",
    "template_filename = \"tradrz_soft_template_simple.txt\"\n",
    "init_from_vocab = True\n",
    "num_soft_tokens = 10\n",
    "batch_size = 8\n",
    "model = \"t5\" # \"mt5\" or \"t5\"\n",
    "model_name_or_path = \"t5-base\" # \"google/mt5-base\" or \"t5-base\"\n",
    "max_seq_len = 32\n",
    "\n",
    "if model == \"mt5\":\n",
    "    plms._MODEL_CLASSES[\"mt5\"]= ModelClass(**{\"config\": MT5Config, \"tokenizer\": MT5Tokenizer, \\\n",
    "                                              \"model\": MT5ForConditionalGeneration, \"wrapper\": T5TokenizerWrapper})\n",
    "\n",
    "store_dir = \"saved_models_openprompt/\"\n",
    "if not os.path.exists(store_dir):\n",
    "    os.makedirs(store_dir)\n",
    "\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(model, model_name_or_path)\n",
    "\n",
    "# We need to add special tokens for mT5 since it does not have them by default\n",
    "if model == \"mt5\":\n",
    "    tokenizer.add_tokens([\"<extra_id_0>\"])\n",
    "    plm.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.additional_special_tokens = [\"<extra_id_0>\"]\n",
    "    tokenizer.additional_special_token_ids = [len(tokenizer)]\n",
    "  \n",
    "# Preparing the template and the verbalizer\n",
    "# mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=num_soft_tokens, \\\n",
    "#                          initialize_from_vocab=init_from_vocab).from_file(template_filename, choice=0)\n",
    "mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=num_soft_tokens, \\\n",
    "                          initialize_from_vocab=init_from_vocab, text=\"{'placeholder':'text_a'} {'mask'}\")\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=8, \\\n",
    "                                label_words=[[\"Absage\"], [\"Einsatzbefehl\"], [\"Informieren\"], [\"Nachfragen\"], \\\n",
    "                                             [\"Anruf\"], [\"Antwort\"], [\"Sonstiges\"], [\"Zusage\"]])\n",
    "# Preparing the training data\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \\\n",
    "                                    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_len, \\\n",
    "                                    decoder_max_length=5, batch_size=batch_size, shuffle=True, \\\n",
    "                                    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\")\n",
    "\n",
    "# Initializing the prompt model\n",
    "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "prompt_model = prompt_model.to(device)\n",
    "\n",
    "# Setting up the loss function and the optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "\n",
    "# Preparing the vaildation data\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer, \\\n",
    "                                         tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_len, decoder_max_length=5, \\\n",
    "                                         batch_size=batch_size,shuffle=False, teacher_forcing=False, predict_eos_token=False, \\\n",
    "                                         truncate_method=\"head\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae920014-664c-466c-ba37-5dddf8c76099",
   "metadata": {},
   "source": [
    "**How to switch between the hard and soft (trainable) prompts?**\n",
    "\n",
    "OpenPrompt provides some [examples](https://thunlp.github.io/OpenPrompt/notes/template.html) to create the templates with both types of prompts. Note that soft and hard prompts can also be combined in a single template:\n",
    "```\n",
    "{\"meta\": \"premise\"} {\"meta\": \"hypothesis\"} {\"soft\": \"Does\"} {\"soft\": \"the\", \"soft_id\": 1} first sentence entails {\"soft_id\": 1} second? {\"mask\"}\n",
    "```\n",
    "Here \"meta\" refers to the original text input, \"mask\" corresponds to the index of the token that needs to be predicted, \"soft\" key denotes soft tokens with trainable embeddings and \"hard\" tokens can be directly written as part of the template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e176f0c-64d2-4dbb-9712-88462993494e",
   "metadata": {},
   "source": [
    "### üöÄ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adba1b4f-e95f-41ff-8d9e-c9096a118641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average training loss: 0.794688139475358\n",
      "Epoch 0, average validation loss: 6.663154400216463\n",
      "Epoch 1, average training loss: 0.3376382673273165\n",
      "Epoch 1, average validation loss: 2.8309670106675\n",
      "Epoch 2, average training loss: 0.1846003383818148\n",
      "Epoch 2, average validation loss: 1.5478028372013704\n",
      "Epoch 3, average training loss: 0.12057039987179095\n",
      "Epoch 3, average validation loss: 1.0109364296942471\n",
      "Epoch 4, average training loss: 0.10880696375456295\n",
      "Epoch 4, average validation loss: 0.912304542249797\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "dev_loss_min = None\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"Epoch {}, average training loss: {}\".format(epoch, total_loss/(step+1)), flush=True)\n",
    "    \n",
    "    # Evaluation on the validation set\n",
    "    with torch.no_grad():\n",
    "        cur_loss = 0\n",
    "        for step, inputs in enumerate(validation_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            logits = prompt_model(inputs)\n",
    "            labels = inputs['label']\n",
    "            loss = loss_func(logits, labels)\n",
    "            cur_loss += loss.item()\n",
    "        cur_loss = cur_loss/(step+1)\n",
    "        if dev_loss_min is None or cur_loss<dev_loss_min:\n",
    "            dev_loss_min = cur_loss\n",
    "            torch.save(prompt_model.state_dict(), store_dir+pmodel_name+\".pt\")\n",
    "    print(\"Epoch {}, average validation loss: {}\".format(epoch, total_loss/(step+1)), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d92c5e-6407-4fd0-b9a4-e7de703ddfcd",
   "metadata": {},
   "source": [
    "### ‚úÖ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aae99a4-df66-4e94-a06d-b50a0d631f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 605it [00:00, 2543.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: tradrz-5-epochs\n",
      "Accuracy: 0.7735537190082644\n",
      "Accuracy: 0.774 matched: 468 total: 605\n",
      "F1 scores:\n",
      "Absage F1: 0.571\n",
      "Einsatzbefehl F1: 0.646\n",
      "Information_geben F1: 0.802\n",
      "Information_nachfragen F1: 0.882\n",
      "Kontakt_Anfrage F1: 0.878\n",
      "Kontakt_Bestaetigung F1: 0.855\n",
      "Sonstiges F1: 0.4\n",
      "Zusage F1: 0.358\n",
      "Macro F1: 0.674\n",
      "Micro F1: 0.774\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \\\n",
    "                                   tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_len, \\\n",
    "                                   decoder_max_length=3, batch_size=batch_size,shuffle=False, \\\n",
    "                                   teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\")\n",
    "\n",
    "prompt_model.load_state_dict(torch.load(store_dir+pmodel_name+\".pt\"))\n",
    "torch.save(prompt_model.template.soft_embeds.data, 'soft_tensors.pt')\n",
    "\n",
    "print(\"Evaluating model:\", pmodel_name)\n",
    "\n",
    "alltexts = []\n",
    "alltexts = [instance.text_a for instance in dataset[\"test\"]]\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "    inputs = inputs.to(device)\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "all_labels = [\"Absage\", \"Einsatzbefehl\", \"Information_geben\", \"Information_nachfragen\", \"Kontakt_Anfrage\", \"Kontakt_Bestaetigung\", \"Sonstiges\", \"Zusage\"]\n",
    "id2label = dict()\n",
    "for i, label in enumerate(all_labels):\n",
    "    id2label[i] = label\n",
    "\n",
    "scores = dict()\n",
    "for label in all_labels:\n",
    "    scores[label] = {\"tp\":0, \"fp\":0, \"fn\":0}\n",
    "\n",
    "match = 0\n",
    "for i in range(len(allpreds)):\n",
    "    predicted_label = id2label[allpreds[i]]\n",
    "    gold_label = id2label[alllabels[i]]\n",
    "    if predicted_label==gold_label:\n",
    "        match+=1\n",
    "        scores[predicted_label][\"tp\"]+=1\n",
    "    else:\n",
    "        #print(\"Predicted:\", predicted_label)\n",
    "        #print(\"Gold:\", gold_label)\n",
    "        #print(\"Text:\", alltexts[i], \"\\n\")\n",
    "        scores[predicted_label][\"fp\"]+=1\n",
    "        scores[gold_label][\"fn\"]+=1\n",
    "\n",
    "print(\"Accuracy:\", round(match/len(allpreds),3), \"matched:\", match, \"total:\", len(allpreds))\n",
    "print(\"F1 scores:\")\n",
    "\n",
    "micro_prec = 0\n",
    "micro_rec = 0\n",
    "micro_f1 = 0\n",
    "\n",
    "f1scores = 0\n",
    "all_tp = 0\n",
    "all_fp = 0\n",
    "all_fn = 0\n",
    "\n",
    "# Computing F1 scores (per label)\n",
    "for label in all_labels:\n",
    "    tp = scores[label][\"tp\"]\n",
    "    fp = scores[label][\"fp\"]\n",
    "    fn = scores[label][\"fn\"]\n",
    "    all_tp+=tp\n",
    "    all_fp+=fp\n",
    "    all_fn+=fn\n",
    "    if (tp+fp)>0:\n",
    "        prec = tp/(tp+fp)\n",
    "    else:\n",
    "        prec = 0\n",
    "    if (tp+fn)>0:\n",
    "        rec = tp/(tp+fn)\n",
    "    else:\n",
    "        rec = 0\n",
    "    if (prec+rec)>0:\n",
    "        f1score = 2*prec*rec/(prec+rec)\n",
    "    else:\n",
    "        f1score = 0\n",
    "    f1scores+=f1score\n",
    "    print(label, \"F1:\", round(f1score,3))\n",
    "if (all_tp+all_fp)>0:\n",
    "    micro_prec = all_tp/(all_tp+all_fp)\n",
    "if (all_tp+all_fn)>0:\n",
    "    micro_rec = all_tp/(all_tp+all_fn)\n",
    "if (micro_prec+micro_rec)>0:\n",
    "    micro_f1 = 2*micro_prec*micro_rec/(micro_prec+micro_rec)\n",
    "\n",
    "# Computing macro F1 scores\n",
    "print(\"Macro F1:\", round(f1scores/len(all_labels),3))\n",
    "print(\"Micro F1:\", round(micro_f1,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
